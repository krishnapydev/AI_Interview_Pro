
import { GoogleGenAI, GenerateContentResponse, Chat, GroundingChunk, GenerateVideosOperation, Modality } from "@google/genai";
import { fileToBase64 } from "../utils";

const getApiKey = () => {
    const key = process.env.API_KEY;
    if (!key) {
        throw new Error("API_KEY environment variable not set.");
    }
    return key;
};

// We create a new instance for each call that needs it, especially for Veo
// to ensure the latest key from the dialog is used.
const getAIClient = () => new GoogleGenAI({ apiKey: getApiKey() });


// Chat functionality
export const createChatSession = (): Chat => {
    const ai = getAIClient();
    return ai.chats.create({ model: 'gemini-2.5-flash' });
};

export const streamChatResponse = async (chat: Chat, message: string, useGrounding: boolean) => {
    const ai = getAIClient();
    if (useGrounding) {
        // This is a simplified approach. A real app might have more sophisticated logic
        // to decide which grounding tool to use.
        const tools = [{ googleSearch: {} }];
        // A single-turn generateContent is better for grounding than chat.
        return ai.models.generateContentStream({
            model: 'gemini-2.5-flash',
            contents: { role: 'user', parts: [{ text: message }] },
            config: { tools }
        });
    }
    return chat.sendMessageStream({ message });
};

// Resume Analysis (Complex Task)
export const analyzeResumeWithJd = async (resumeText: string, jdText: string): Promise<string> => {
    const ai = getAIClient();
    const prompt = `Analyze the following resume against the job description. Provide a detailed analysis including:
    1.  An overall ATS-friendliness score (out of 100).
    2.  A list of key skills and keywords missing from the resume that are present in the job description.
    3.  Suggestions for improving the summary/objective section.
    4.  Actionable advice on how to better tailor the experience section to the job description.
    
    Format the output as a JSON object with keys: "score", "missingKeywords", "summarySuggestion", "experienceAdvice".

    --- RESUME ---
    ${resumeText}

    --- JOB DESCRIPTION ---
    ${jdText}
    `;

    try {
        const response = await ai.models.generateContent({
            model: 'gemini-2.5-pro',
            contents: prompt,
            config: {
                thinkingConfig: { thinkingBudget: 32768 },
                responseMimeType: 'application/json'
            },
        });
        return response.text;
    } catch (error) {
        console.error("Error analyzing resume:", error);
        return JSON.stringify({ error: "Failed to analyze resume. Please try again." });
    }
};


// Image Generation
export const generateImage = async (prompt: string, aspectRatio: string): Promise<string> => {
    const ai = getAIClient();
    const response = await ai.models.generateImages({
        model: 'imagen-4.0-generate-001',
        prompt,
        config: {
            numberOfImages: 1,
            outputMimeType: 'image/jpeg',
            aspectRatio: aspectRatio as "1:1" | "3:4" | "4:3" | "9:16" | "16:9",
        },
    });

    const base64ImageBytes = response.generatedImages[0].image.imageBytes;
    return `data:image/jpeg;base64,${base64ImageBytes}`;
};

// Image Editing
export const editImage = async (imageFile: File, prompt: string): Promise<string> => {
    const ai = getAIClient();
    const base64Data = await fileToBase64(imageFile);
    
    const response = await ai.models.generateContent({
        model: 'gemini-2.5-flash-image',
        contents: {
            parts: [
                { inlineData: { data: base64Data, mimeType: imageFile.type } },
                { text: prompt },
            ],
        },
        config: {
            responseModalities: [Modality.IMAGE],
        },
    });

    for (const part of response.candidates?.[0]?.content.parts ?? []) {
        if (part.inlineData) {
            const base64ImageBytes: string = part.inlineData.data;
            return `data:${part.inlineData.mimeType};base64,${base64ImageBytes}`;
        }
    }
    throw new Error("No image was generated by the model.");
};

// Video Generation
export const generateVideo = async (
    prompt: string,
    aspectRatio: '16:9' | '9:16',
    startImageFile?: File
): Promise<string> => {
    const ai = getAIClient();
    let operation: GenerateVideosOperation;

    if (startImageFile) {
        const imageBytes = await fileToBase64(startImageFile);
        operation = await ai.models.generateVideos({
            model: 'veo-3.1-fast-generate-preview',
            prompt,
            image: { imageBytes, mimeType: startImageFile.type },
            config: {
                numberOfVideos: 1,
                resolution: '720p',
                aspectRatio,
            }
        });
    } else {
        operation = await ai.models.generateVideos({
            model: 'veo-3.1-fast-generate-preview',
            prompt,
            config: {
                numberOfVideos: 1,
                resolution: '720p',
                aspectRatio,
            }
        });
    }
    
    while (!operation.done) {
        console.log("Polling video generation status...");
        await new Promise(resolve => setTimeout(resolve, 10000)); // Poll every 10 seconds
        operation = await ai.operations.getVideosOperation({ operation: operation });
    }

    const downloadLink = operation.response?.generatedVideos?.[0]?.video?.uri;
    if (!downloadLink) {
        throw new Error("Video generation failed or returned no URI.");
    }
    
    const videoResponse = await fetch(`${downloadLink}&key=${getApiKey()}`);
    if (!videoResponse.ok) {
        throw new Error(`Failed to fetch video: ${videoResponse.statusText}`);
    }
    const videoBlob = await videoResponse.blob();
    return URL.createObjectURL(videoBlob);
};

// TTS
export const textToSpeech = async (text: string): Promise<AudioBuffer> => {
    const ai = getAIClient();
    const response = await ai.models.generateContent({
        model: "gemini-2.5-flash-preview-tts",
        contents: [{ parts: [{ text: `Say this naturally: ${text}` }] }],
        config: {
            responseModalities: [Modality.AUDIO],
            speechConfig: {
                voiceConfig: {
                    prebuiltVoiceConfig: { voiceName: 'Kore' },
                },
            },
        },
    });
    
    const base64Audio = response.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
    if (!base64Audio) {
        throw new Error("TTS generation failed.");
    }

    const { decode, decodeAudioData } = await import("../utils");
    const outputAudioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });
    const audioBuffer = await decodeAudioData(
        decode(base64Audio),
        outputAudioContext,
        24000,
        1,
    );
    return audioBuffer;
};

export const playAudioBuffer = (audioBuffer: AudioBuffer) => {
    const outputAudioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 24000 });
    const source = outputAudioContext.createBufferSource();
    source.buffer = audioBuffer;
    source.connect(outputAudioContext.destination);
    source.start();
}
